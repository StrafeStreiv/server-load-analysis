import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')


class AdvancedAnalyzer:
    def __init__(self, db_connector):
        self.db = db_connector

    def detect_anomalies(self, df, method='zscore', threshold=3):
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
        anomalies = pd.DataFrame()

        if method == 'zscore':
            # Z-score –º–µ—Ç–æ–¥
            numeric_cols = ['cpu_usage', 'memory_usage', 'response_time_ms', 'error_rate']
            for col in numeric_cols:
                if col in df.columns:
                    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
                    col_anomalies = df[z_scores > threshold]
                    anomalies = pd.concat([anomalies, col_anomalies])

        elif method == 'iqr':
            # IQR –º–µ—Ç–æ–¥
            numeric_cols = ['cpu_usage', 'memory_usage', 'response_time_ms', 'error_rate']
            for col in numeric_cols:
                if col in df.columns:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    col_anomalies = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                    anomalies = pd.concat([anomalies, col_anomalies])

        return anomalies.drop_duplicates()

    def cluster_servers(self, df):
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–µ—Ä–æ–≤ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –Ω–∞–≥—Ä—É–∑–∫–∏"""
        features = df.groupby('server_id').agg({
            'cpu_usage': ['mean', 'std', 'max'],
            'memory_usage': ['mean', 'std'],
            'response_time_ms': ['mean', 'max'],
            'error_rate': 'mean'
        }).round(3)

        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏
        features.columns = ['_'.join(col).strip() for col in features.columns]

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(features)

        # K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        kmeans = KMeans(n_clusters=min(3, len(features)), random_state=42)
        clusters = kmeans.fit_predict(scaled_features)

        features['cluster'] = clusters

        print("üìä Server Clustering Results:")
        for cluster_id in range(len(set(clusters))):
            cluster_servers = features[features['cluster'] == cluster_id]
            print(f"Cluster {cluster_id}: {', '.join(cluster_servers.index.tolist())}")

        return features

    def forecast_load(self, df, hours=6):
        """–ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏"""
        df = df.copy()
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df.set_index('timestamp', inplace=True)

        # –†–µ—Å–µ–º–ø–ª–∏—Ä—É–µ–º –ø–æ —á–∞—Å–∞–º
        hourly_load = df['cpu_usage'].resample('H').mean()

        # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
        forecast = hourly_load.rolling(window=3).mean().iloc[-hours:]

        print("üîÆ Load Forecast (next 6 hours):")
        for time, load in forecast.items():
            print(f"  {time.strftime('%H:%M')}: {load:.1f}% CPU")

        return forecast

    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print("üìà ADVANCED ANALYSIS REPORT")
        print("=" * 50)

        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        metrics_df = self.db.get_server_metrics()

        if metrics_df.empty:
            print("No data available for analysis")
            return

        # 1. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        print("\n1. üö® ANOMALY DETECTION")
        anomalies = self.detect_anomalies(metrics_df)
        print(f"Found {len(anomalies)} anomalous records")
        if not anomalies.empty:
            print("Top anomalies:")
            print(anomalies[['timestamp', 'server_id', 'cpu_usage', 'response_time_ms']].head())

        # 2. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–µ—Ä–æ–≤
        print("\n2. üéØ SERVER CLUSTERING")
        clusters = self.cluster_servers(metrics_df)

        # 3. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏
        print("\n3. üîÆ LOAD FORECASTING")
        forecast = self.forecast_load(metrics_df)

        # 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print("\n4. üí° RECOMMENDATIONS")
        self.generate_recommendations(metrics_df)

    def generate_recommendations(self, df):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        server_stats = df.groupby('server_id').agg({
            'cpu_usage': ['mean', 'max'],
            'memory_usage': ['mean', 'max'],
            'response_time_ms': 'mean',
            'error_rate': 'mean'
        }).round(2)

        print("Optimization recommendations:")

        for server in server_stats.index:
            avg_cpu = server_stats.loc[server, ('cpu_usage', 'mean')]
            max_cpu = server_stats.loc[server, ('cpu_usage', 'max')]
            avg_response = server_stats.loc[server, ('response_time_ms', 'mean')]

            if avg_cpu > 70:
                print(f"  ‚ö†Ô∏è  {server}: High average CPU ({avg_cpu}%) - consider scaling")
            elif max_cpu > 90:
                print(f"  üî• {server}: CPU spikes detected (up to {max_cpu}%) - optimize peak load")

            if avg_response > 200:
                print(f"  üêå {server}: Slow response time ({avg_response}ms) - investigate bottlenecks")