import pandas as pd
import numpy as np
import scipy.stats as stats
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings('ignore')


class StatisticalEvaluator:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
    –í–∫–ª—é—á–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –ø–æ–ª–Ω–æ—Ç—ã, –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç–∏, –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç–∏ –∏ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
    """

    def __init__(self, confidence_level: float = 0.95):
        self.confidence_level = confidence_level
        self.results = {}

    def evaluate_data_completeness(self, df: pd.DataFrame, time_column: str = 'timestamp') -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df - DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        time_column - –∫–æ–ª–æ–Ω–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ–ª–Ω–æ—Ç—ã
        """
        print("üìä Evaluating data completeness...")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        df = df.copy()
        df[time_column] = pd.to_datetime(df[time_column])
        df.set_index(time_column, inplace=True)

        completeness_metrics = {}

        # 1. –û–±—â–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –ø–æ –∑–∞–ø–∏—Å—è–º
        total_expected = self._calculate_expected_records(df)
        total_actual = len(df)
        completeness_pct = (total_actual / total_expected) * 100 if total_expected > 0 else 0

        completeness_metrics['total_expected'] = total_expected
        completeness_metrics['total_actual'] = total_actual
        completeness_metrics['completeness_percentage'] = round(completeness_pct, 2)

        # 2. –ü–æ–ª–Ω–æ—Ç–∞ –ø–æ —Å–µ—Ä–≤–µ—Ä–∞–º
        if 'server_id' in df.columns:
            server_completeness = {}
            for server in df['server_id'].unique():
                server_data = df[df['server_id'] == server]
                server_expected = self._calculate_expected_records(server_data, is_subset=True)
                server_actual = len(server_data)
                server_pct = (server_actual / server_expected) * 100 if server_expected > 0 else 0
                server_completeness[server] = {
                    'expected': server_expected,
                    'actual': server_actual,
                    'completeness': round(server_pct, 2)
                }

            completeness_metrics['server_completeness'] = server_completeness

        # 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
        self._visualize_missing_data(df)

        print(f"‚úÖ Data completeness: {completeness_pct:.1f}%")

        self.results['completeness'] = completeness_metrics
        return completeness_metrics

    def _calculate_expected_records(self, df: pd.DataFrame, is_subset: bool = False) -> int:
        """–†–∞—Å—á–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π"""
        if len(df) < 2:
            return len(df)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∑–∞–ø–∏—Å—è–º–∏
        time_diffs = df.index.to_series().diff().dropna()
        if len(time_diffs) == 0:
            return len(df)

        # –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–º–æ–¥–∞)
        mode_interval = time_diffs.mode()[0] if not time_diffs.mode().empty else time_diffs.iloc[0]

        # –û–±—â–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫
        time_span = df.index.max() - df.index.min()

        # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
        if mode_interval.total_seconds() > 0:
            expected = (time_span.total_seconds() / mode_interval.total_seconds()) + 1
        else:
            expected = len(df)

        return int(expected)

    def evaluate_sample_homogeneity(self, df: pd.DataFrame, metric_column: str = 'cpu_usage') -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df - DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        metric_column - –∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–æ–≤ –Ω–∞ –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å
        """
        print("üîç Evaluating sample homogeneity...")

        homogeneity_metrics = {}

        if 'server_id' not in df.columns or metric_column not in df.columns:
            print("‚ùå Required columns missing for homogeneity test")
            return homogeneity_metrics

        servers = df['server_id'].unique()
        if len(servers) < 2:
            print("‚ùå Need at least 2 servers for homogeneity test")
            return homogeneity_metrics

        # 1. –¢–µ—Å—Ç ANOVA –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–∏—Ö
        groups = [df[df['server_id'] == server][metric_column].dropna().values
                  for server in servers]

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å (—Ç–µ—Å—Ç –®–∞–ø–∏—Ä–æ-–£–∏–ª–∫–∞)
            normality_results = {}
            for i, server in enumerate(servers):
                if len(groups[i]) >= 3 and len(groups[i]) <= 5000:
                    stat, p_value = stats.shapiro(groups[i])
                    normality_results[server] = {
                        'statistic': round(stat, 4),
                        'p_value': round(p_value, 4),
                        'is_normal': p_value > 0.05
                    }

            homogeneity_metrics['normality_test'] = normality_results

            # ANOVA –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ
            normal_groups = [normality_results.get(s, {}).get('is_normal', False)
                             for s in servers]

            if all(normal_groups) and len(groups) >= 2:
                f_stat, p_value = stats.f_oneway(*groups)
                homogeneity_metrics['anova'] = {
                    'f_statistic': round(f_stat, 4),
                    'p_value': round(p_value, 4),
                    'homogeneous': p_value > 0.05
                }
                print(f"   ANOVA: F={f_stat:.2f}, p={p_value:.4f}, "
                      f"Homogeneous: {p_value > 0.05}")

            # –ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç –ö—Ä—É—Å–∫–∞–ª–∞-–£–æ–ª–ª–∏—Å–∞
            h_stat, p_value = stats.kruskal(*[g for g in groups if len(g) > 0])
            homogeneity_metrics['kruskal_wallis'] = {
                'h_statistic': round(h_stat, 4),
                'p_value': round(p_value, 4),
                'homogeneous': p_value > 0.05
            }
            print(f"   Kruskal-Wallis: H={h_stat:.2f}, p={p_value:.4f}, "
                  f"Homogeneous: {p_value > 0.05}")

            # 2. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
            self._visualize_distributions(df, metric_column)

        except Exception as e:
            print(f"‚ùå Error in homogeneity tests: {e}")

        self.results['homogeneity'] = homogeneity_metrics
        return homogeneity_metrics

    def calculate_confidence_intervals(self, df: pd.DataFrame,
                                       metric_columns: List[str] = None) -> Dict:
        """
        –†–∞—Å—á–µ—Ç –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –¥–ª—è –º–µ—Ç—Ä–∏–∫

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        df - DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        metric_columns - —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏
        """
        print("üéØ Calculating confidence intervals...")

        if metric_columns is None:
            metric_columns = ['cpu_usage', 'memory_usage', 'response_time_ms', 'error_rate']

        confidence_intervals = {}

        for column in metric_columns:
            if column not in df.columns:
                continue

            data = df[column].dropna()
            if len(data) < 2:
                continue

            # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            mean = np.mean(data)
            std = np.std(data, ddof=1)  # –í—ã–±–æ—Ä–æ—á–Ω–æ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
            n = len(data)

            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ t-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            alpha = 1 - self.confidence_level
            t_critical = stats.t.ppf(1 - alpha / 2, df=n - 1)

            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—à–∏–±–∫–∞
            se = std / np.sqrt(n)

            # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            margin_of_error = t_critical * se
            ci_lower = mean - margin_of_error
            ci_upper = mean + margin_of_error

            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å (%)
            relative_error = (margin_of_error / mean) * 100 if mean != 0 else 0

            confidence_intervals[column] = {
                'mean': round(mean, 4),
                'std_dev': round(std, 4),
                'sample_size': n,
                'confidence_level': self.confidence_level,
                't_critical': round(t_critical, 4),
                'standard_error': round(se, 4),
                'margin_of_error': round(margin_of_error, 4),
                'ci_lower': round(ci_lower, 4),
                'ci_upper': round(ci_upper, 4),
                'relative_error_percent': round(relative_error, 2)
            }

            print(f"   {column}: {mean:.2f} ¬± {margin_of_error:.2f} "
                  f"({ci_lower:.2f} - {ci_upper:.2f}) "
                  f"Error: {relative_error:.1f}%")

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
        self._visualize_confidence_intervals(confidence_intervals)

        self.results['confidence_intervals'] = confidence_intervals
        return confidence_intervals

    def calculate_error_metrics(self, predictions: np.ndarray,
                                actuals: np.ndarray) -> Dict:
        """
        –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        predictions - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        actuals - —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç–∏
        """
        print("üìà Calculating error metrics...")

        if len(predictions) != len(actuals):
            raise ValueError("Predictions and actuals must have same length")

        # –£–±–∏—Ä–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        mask = ~np.isnan(predictions) & ~np.isnan(actuals)
        predictions = predictions[mask]
        actuals = actuals[mask]

        if len(predictions) == 0:
            return {}

        error_metrics = {}

        # –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏
        errors = predictions - actuals
        absolute_errors = np.abs(errors)

        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        error_metrics['mae'] = np.mean(absolute_errors)  # MAE
        error_metrics['mse'] = np.mean(errors ** 2)  # MSE
        error_metrics['rmse'] = np.sqrt(error_metrics['mse'])  # RMSE

        # –°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAPE)
        mape_mask = actuals != 0
        if np.any(mape_mask):
            mape = np.mean(np.abs(errors[mape_mask] / actuals[mape_mask])) * 100
            error_metrics['mape'] = mape

        # R-–∫–≤–∞–¥—Ä–∞—Ç (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏)
        ss_res = np.sum(errors ** 2)
        ss_tot = np.sum((actuals - np.mean(actuals)) ** 2)
        if ss_tot != 0:
            error_metrics['r_squared'] = 1 - (ss_res / ss_tot)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –î–∞—Ä–±–∏–Ω–∞-–í–∞—Ç—Å–æ–Ω–∞ (–∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –æ—à–∏–±–æ–∫)
        if len(errors) > 1:
            dw = np.sum(np.diff(errors) ** 2) / np.sum(errors ** 2)
            error_metrics['durbin_watson'] = dw

        # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ
        for key in error_metrics:
            error_metrics[key] = round(error_metrics[key], 4)

        print(f"   MAE: {error_metrics.get('mae', 0):.4f}, "
              f"RMSE: {error_metrics.get('rmse', 0):.4f}, "
              f"R¬≤: {error_metrics.get('r_squared', 0):.4f}")

        self.results['error_metrics'] = error_metrics
        return error_metrics

    def _visualize_missing_data(self, df: pd.DataFrame):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        plt.figure(figsize=(12, 6))

        # Heatmap –ø—Ä–æ–ø—É—Å–∫–æ–≤
        if len(df) > 0:
            missing_data = df.isnull()
            sns.heatmap(missing_data, cbar=False, cmap='viridis')
            plt.title('Missing Data Pattern (Yellow = Missing)')
            plt.tight_layout()
            plt.savefig('reports/missing_data_heatmap.png', dpi=150, bbox_inches='tight')
            plt.close()

    def _visualize_distributions(self, df: pd.DataFrame, metric_column: str):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ø–æ —Å–µ—Ä–≤–µ—Ä–∞–º"""
        if 'server_id' not in df.columns:
            return

        plt.figure(figsize=(10, 6))

        servers = df['server_id'].unique()
        colors = plt.cm.Set3(np.linspace(0, 1, len(servers)))

        for i, server in enumerate(servers):
            server_data = df[df['server_id'] == server][metric_column].dropna()
            if len(server_data) > 0:
                # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å —è–¥–µ—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
                sns.histplot(server_data, kde=True, alpha=0.5,
                             label=server, color=colors[i], bins=20)

        plt.title(f'Distribution of {metric_column} by Server')
        plt.xlabel(metric_column)
        plt.ylabel('Frequency')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('reports/distribution_comparison.png', dpi=150, bbox_inches='tight')
        plt.close()

    def _visualize_confidence_intervals(self, confidence_intervals: Dict):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤"""
        if not confidence_intervals:
            return

        metrics = list(confidence_intervals.keys())
        means = [ci['mean'] for ci in confidence_intervals.values()]
        lowers = [ci['ci_lower'] for ci in confidence_intervals.values()]
        uppers = [ci['ci_upper'] for ci in confidence_intervals.values()]
        errors = [ci['margin_of_error'] for ci in confidence_intervals.values()]

        plt.figure(figsize=(10, 6))

        # –°—Ç–æ–ª–±—Ü—ã —Å–æ —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        bars = plt.bar(metrics, means, yerr=errors, capsize=10,
                       alpha=0.7, color='skyblue', edgecolor='black')

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, mean_val in zip(bars, means):
            plt.text(bar.get_x() + bar.get_width() / 2., bar.get_height() + 0.1,
                     f'{mean_val:.2f}', ha='center', va='bottom', fontsize=9)

        plt.title(f'Confidence Intervals ({self.confidence_level * 100:.0f}% Confidence Level)')
        plt.ylabel('Metric Value')
        plt.grid(True, alpha=0.3, axis='y')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('reports/confidence_intervals.png', dpi=150, bbox_inches='tight')
        plt.close()

    def generate_statistical_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤—ã–≤–æ–¥–∞–º–∏"""
        report_lines = []

        report_lines.append("=" * 60)
        report_lines.append("STATISTICAL EVALUATION REPORT")
        report_lines.append("=" * 60)

        # 1. –ü–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
        if 'completeness' in self.results:
            comp = self.results['completeness']
            report_lines.append("\n1. DATA COMPLETNESS")
            report_lines.append("-" * 40)
            report_lines.append(f"Total expected records: {comp.get('total_expected', 0):,}")
            report_lines.append(f"Total actual records: {comp.get('total_actual', 0):,}")
            report_lines.append(f"Completeness: {comp.get('completeness_percentage', 0):.1f}%")

            if 'server_completeness' in comp:
                report_lines.append("\nCompleteness by server:")
                for server, stats in comp['server_completeness'].items():
                    report_lines.append(f"  {server}: {stats['completeness']}% "
                                        f"({stats['actual']}/{stats['expected']} records)")

        # 2. –û–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å
        if 'homogeneity' in self.results:
            homo = self.results['homogeneity']
            report_lines.append("\n2. SAMPLE HOMOGENEITY")
            report_lines.append("-" * 40)

            if 'kruskal_wallis' in homo:
                kw = homo['kruskal_wallis']
                report_lines.append(f"Kruskal-Wallis Test:")
                report_lines.append(f"  H-statistic: {kw['h_statistic']}")
                report_lines.append(f"  p-value: {kw['p_value']}")
                report_lines.append(f"  Samples are homogeneous: {kw['homogeneous']}")

        # 3. –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
        if 'confidence_intervals' in self.results:
            cis = self.results['confidence_intervals']
            report_lines.append("\n3. CONFIDENCE INTERVALS")
            report_lines.append("-" * 40)

            for metric, ci in cis.items():
                report_lines.append(f"{metric.upper()}:")
                report_lines.append(f"  Mean: {ci['mean']:.4f} ¬± {ci['margin_of_error']:.4f}")
                report_lines.append(f"  95% CI: [{ci['ci_lower']:.4f}, {ci['ci_upper']:.4f}]")
                report_lines.append(f"  Relative error: {ci['relative_error_percent']}%")
                report_lines.append(f"  Sample size: {ci['sample_size']}")

        # 4. –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç–∏
        if 'error_metrics' in self.results:
            errors = self.results['error_metrics']
            report_lines.append("\n4. ERROR METRICS")
            report_lines.append("-" * 40)

            for metric, value in errors.items():
                report_lines.append(f"  {metric.upper()}: {value}")

        report_lines.append("\n" + "=" * 60)
        report_lines.append("Formulas used:")
        report_lines.append("- Confidence Interval: xÃÑ ¬± t*(s/‚àön)")
        report_lines.append("- Standard Error: s/‚àön")
        report_lines.append("- Relative Error: (Margin of Error / Mean) * 100%")
        report_lines.append("- Kruskal-Wallis: Non-parametric test for homogeneity")
        report_lines.append("=" * 60)

        report = '\n'.join(report_lines)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
        with open('reports/statistical_evaluation.txt', 'w', encoding='utf-8') as f:
            f.write(report)

        print("‚úÖ Statistical report generated: reports/statistical_evaluation.txt")

        return report


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def demonstrate_statistical_evaluation():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ü–µ–Ω—â–∏–∫–∞"""
    print("üß™ Demonstrating statistical evaluation...")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', '2024-01-10', freq='1H')

    data = []
    for i, date in enumerate(dates):
        for server in ['web01', 'api01', 'db01']:
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
            if np.random.random() > 0.05:  # 5% –ø—Ä–æ–ø—É—Å–∫–æ–≤
                data.append({
                    'timestamp': date,
                    'server_id': server,
                    'cpu_usage': np.random.normal(50, 15),
                    'memory_usage': np.random.normal(60, 10),
                    'response_time_ms': np.random.normal(100, 30),
                    'error_rate': np.random.exponential(1)
                })

    df = pd.DataFrame(data)

    # –°–æ–∑–¥–∞–µ–º –æ—Ü–µ–Ω—â–∏–∫ –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
    evaluator = StatisticalEvaluator(confidence_level=0.95)

    # 1. –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã
    completeness = evaluator.evaluate_data_completeness(df)

    # 2. –û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç–∏
    homogeneity = evaluator.evaluate_sample_homogeneity(df, 'cpu_usage')

    # 3. –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
    confidence_intervals = evaluator.calculate_confidence_intervals(df)

    # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report = evaluator.generate_statistical_report()

    print("\nüìä Key statistical insights for presentation:")
    print("1. Data completeness metrics and missing patterns")
    print("2. Statistical tests for sample homogeneity")
    print("3. Confidence intervals with error margins")
    print("4. Formulas: CI = xÃÑ ¬± t*(s/‚àön), R¬≤, MAE, RMSE")

    return evaluator


if __name__ == "__main__":
    evaluator = demonstrate_statistical_evaluation()